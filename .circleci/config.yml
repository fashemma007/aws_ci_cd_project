version: 2.1
# USE A PACKAGE OF CONFIGURATION CALLED AN ORB.
orbs:
  #CHOOSE EITHER ONE OF THE ORBS BELOW  
  aws-cli: circleci/aws-cli@2.0.3
  ## aws-cli: circleci/aws-cli@2.0.3
  ## welcome: circleci/welcome-orb@0.4.1

#AWS parameters for dynamic workflows
## NOTE: Remember to use the same instance-name used in template.yml
parameters:
  stack-name:
    type: string
    default: backend-${CIRCLE_WORKFLOW_ID:0:5}
  instance-name:
    type: string
    default: "ansible-instance"

# ROLLBACK CONFIGURATION
commands:
  destroy_environment:
    steps:
      - run:
          name:  Destroy Environments
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name << pipeline.parameters.stack-name >>
  

# ORCHESTRATE OR SCHEDULE A SET OF JOBS
jobs:
  # Exercise: Creating a Simple Workflow
  # Exercise: Environment Variables
  # Exercise: Reusable Job Code 
  print_greetings:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - print_pipeline_id:
          id: << pipeline.id >>
      - run: echo HELLO
      - run: echo WORLD
      - run: echo $_env_name

  # EXERCISE: INFRASTRUCTURE CREATION
  create_infrastructure: 
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
              --template-file template.yml \
              --stack-name << pipeline.parameters.stack-name >> --region us-east-1
## Note: The below was commented out to keep a unique and constant stack name
# --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5} \
              # --region us-east-1
              # ansible-cloudformation-stack

# AUTOMATED INSTANCE-IP RETRIEVAL
  adding_ips:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - run:
          name: Add instance ip to ansible inventory
          command: |
            aws ec2 describe-instances --query 'Reservations[*].Instances[*].PublicIpAddress' --filters  "Name=tag:Name,Values= << pipeline.parameters.instance-name >>" --output text >> inventory.txt
      - persist_to_workspace:
          root: ~/
          paths:
            - project/
            

# EXERCISE: CONFIG AND DEPLOYMENT
  configure_infrastructure: 
    docker:
      - image: python:3.10-alpine3.16
    steps:
      - checkout
      - add_ssh_keys:
              fingerprints: ["17:03:c5:7f:62:60:69:97:e0:50:ba:8e:69:e7:20:b5"] ##used ed25519 key-pair type named "test"
      - attach_workspace:
          at: ~/
      
      - run: 
          name: View content of inventory.txt
          command: cat inventory.txt 
      
      - run:
          name: Install Ansible
          command: |
            ## Install Ansible
              apk add --update ansible
      - run:
          name: Run Playbook and Configure server
          command: |
            ansible-playbook -i inventory.txt main-remote.yml --key-file ~/.ssh/id_rsa_1703c57f62606997e050ba8e69e720b5 -vvvv
  
  # EXERCISE: SMOKE TESTING
  smoke_test:
    docker:
      # Lightweight Docker image
      - image: python:3.10-alpine3.16
    steps:
      # Checkout code from git
      - checkout
      - run:
          name: Install dependencies such as curl, aws-cli,nodejs and npm
          command: |
            apk add --update --no-cache curl aws-cli tar gzip nodejs npm
      - attach_workspace:
          at: ~/
      - run:
          name: Backend smoke test.
          command: |
            export BACKEND_IP=$(aws ec2 describe-instances \
            --query 'Reservations[*].Instances[*].PublicIpAddress' \
            --filters Name=tag:Name,Values=<< pipeline.parameters.instance-name >> \
            --output text)
            export API_URL="http://${BACKEND_IP}:3000"
            echo "${API_URL}"
            if curl -s "${API_URL}"
            then
                return 0 #change 0 to 1 to intentionally fail
            else
                return 1
            fi
      - destroy_environment
  
  #EXERCISE: PROMOTE TO PRODUCTION
  create_and_deploy_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute bucket.yml - Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
            --template-file bucket.yml \
            --stack-name create-bucket-${CIRCLE_WORKFLOW_ID:0:7} \
            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
      # Uncomment the step below if yoou wish to upload all contents of the current directory to the S3 bucket
      - run: aws s3 sync . s3://mybucket-${CIRCLE_WORKFLOW_ID:0:7} --delete

  
  
  # Fetch and save the pipeline ID (bucket ID) responsible for the last release.
  get_last_deployment_id:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - run:
          name: Fetch and save the old pipeline ID (bucket name) responsible for the last release.
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
            --no-paginate --output text > ~/textfile.txt
      - persist_to_workspace:
          root: ~/
          paths: 
            - textfile.txt 

  # Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its target from the old bucket to the new bucket - `mybucket-${CIRCLE_WORKFLOW_ID:0:7}`
  # Notice here we use the stack name `production-distro` which is the same name we used while deploying to the S3 bucket manually.
  promote_to_production:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute cloudfront.yml
          command: |
            aws cloudformation deploy \
            --template-file cloudfront.yml \
            --stack-name production-distro \
            --parameter-overrides PipelineID="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
  # Destroy the previous production version's S3 bucket and CloudFormation stack. 
  clean_up_old_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Destroy the previous S3 bucket and CloudFormation stack. 
          # Use $OldBucketID environment variable or mybucket644752792305 below.
          # Similarly, you can create and use $OldStackID environment variable in place of production-distro 
          command: |
            export OldBucketID=$(cat ~/textfile.txt)
            aws s3 rm "s3://${OldBucketID}" --recursive

# WORKFLOWS
workflows:
  # Name the workflow "welcome"
  auto_deploy_workflow:
    # Run the welcome/run job in its own container
    jobs:
      - create_infrastructure
      
      - adding_ips:
          requires: [create_infrastructure]
      
      - configure_infrastructure: 
          requires: [adding_ips]
      
      - smoke_test:
          requires: [configure_infrastructure]
      
      - create_and_deploy_front_end
      
      - promote_to_production:
          requires: 
            - create_and_deploy_front_end
      
      - get_last_deployment_id
      
      - clean_up_old_front_end:
          requires:
            - get_last_deployment_id
            - promote_to_production